{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XQByno9CIUg"
   },
   "source": [
    "#### <b>Download the Dataset</b>\n",
    "\n",
    "* The dataset contains approximately 13,000 Korean \"human face\" images.\n",
    "* In this dataset, all faces are cropped to a resolution of 128 X 128 pixels, although some of the original images have been high resolution.\n",
    "* Each image filename indicates which family (household) number it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWvuBOPACCza",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/EbMhBPnmIb5MutZvGicPKggBWKm5hLs0iwKfGW7_TwQIKg?download=1 -O custom_korean_family_dataset_resolution_128.zip\n",
    "!unzip custom_korean_family_dataset_resolution_128.zip -d ./custom_korean_family_dataset_resolution_128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7TT4oZ3DR_j"
   },
   "source": [
    "#### <b>Load Libraries</b>\n",
    "\n",
    "* Load various useful python libraries for the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pillow\n",
    "# ! pip install matplotlib\n",
    "# ! pip install pandas\n",
    "# ! pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# ! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJl1GEjIDTuA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn import linear_model, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed=0):\n",
    "    torch.manual_seed(seed + 0)\n",
    "    torch.cuda.manual_seed(seed + 1)\n",
    "    torch.cuda.manual_seed_all(seed + 2)\n",
    "    np.random.seed(seed + 3)\n",
    "    torch.cuda.manual_seed_all(seed + 4)\n",
    "    random.seed(seed + 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JD5_JulRpIdq"
   },
   "source": [
    "#### <b>Family Relationship Dataset</b>\n",
    "\n",
    "1. It consists of a total of 900 households and approximately exact 13,068 individuals.\n",
    "2. The last part of the filename, such as \"a1\", indicates the \"age group\".\n",
    "  * This dataset consists of 8 age group classes.\n",
    "  * The task is one of the 8-classes classification problem.\n",
    "  * If we perform random guessing, the accuracy is approximately 12.5%.\n",
    "3. The age classification problem is somewhat challenging, thus, we expect that our dataset will be helpful for evaluating the forgetting performance of a machine unlearning algorithm.\n",
    "  * Our proposed setting has been configured in a way that is almost similar to the NeurIPS Kaggle Competition 2023 held on Kaggle.\n",
    "4. We have utilized \"Family Relationship Dataset\" from AI Hub for constructing this dataset.\n",
    "  * <b>Link</b>: https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=&topMenu=&aihubDataSe=realm&dataSetSn=528"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkbDGUSECOC7"
   },
   "source": [
    "#### <b>Load Datasets 1</b>\n",
    "\n",
    "* The following three datasets do not overlap at the household level.\n",
    "  * Thus, our setting ensures any individual \"subjects\" are overlapped across the following three sub-dataset.\n",
    "  * In other wrods, we set any person (subject) to do not simultaneously belonging to the $\\mathcal{D}_{train}$, $\\mathcal{D}_{test}$ and $\\mathcal{D}_{unseen}$.\n",
    "* <b>Training dataset $\\mathcal{D}_{train}$</b>: (F0001 ~ F0299) folders have 10,025 images.\n",
    "* <b>Test dataset $\\mathcal{D}_{test}$</b>: (F0801 ~ F0850) folders have 1,539 images.\n",
    "* <b>Unseen dataset $\\mathcal{D}_{unseen}$</b>: (F0851 ~ F0900) folders have 1,504 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Be8TVf3nDOBF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "> [Function] Parse the metadata.\n",
    "* image_age_list[0] = [\"F0001_AGE_D_18_a1.jpg\"] = \"a\"\n",
    "* image_age_list[1] = [\"F0001_AGE_D_18_a2.jpg\"] = \"a\"\n",
    "* image_age_list[2] = [\"F0001_AGE_D_18_a3.jpg\"] = \"a\"\n",
    "* image_age_list[3] = [\"F0001_AGE_D_18_a4.jpg\"] = \"a\"\n",
    "* image_age_list[4] = [\"F0001_AGE_D_18_b1.jpg\"] = \"b\"\n",
    "...\n",
    "\"\"\"\n",
    "def parsing(meta_data):\n",
    "    image_age_list = []\n",
    "    # iterate all rows in the metadata file\n",
    "    for idx, row in meta_data.iterrows():\n",
    "        image_path = row['image_path']\n",
    "        age_class = row['age_class']\n",
    "        image_age_list.append([image_path, age_class])\n",
    "    return image_age_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOWMezXeDbnu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, meta_data, image_directory, transform=None, forget=False, retain=False, shadow_idx=-1):\n",
    "        self.meta_data = meta_data\n",
    "        self.image_directory = image_directory\n",
    "        self.transform = transform\n",
    "\n",
    "        # Process the metadata.\n",
    "        image_age_list = parsing(meta_data)\n",
    "\n",
    "        self.image_age_list = image_age_list\n",
    "        self.age_class_to_label = {\n",
    "            \"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3, \"e\": 4, \"f\": 5, \"g\": 6, \"h\": 7\n",
    "        }\n",
    "\n",
    "        # After training the original model, we will do \"machine unlearning\".\n",
    "        # The machine unlearning requires two datasets, ① forget dataset and ② retain dataset.\n",
    "        # In this experiment, we set the first 1,500 images to be forgotten and the rest images to be retained.\n",
    "        if forget:\n",
    "            self.image_age_list = self.image_age_list[:1500]\n",
    "        if retain:\n",
    "            self.image_age_list = self.image_age_list[1500:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_age_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, age_class = self.image_age_list[idx]\n",
    "        img = Image.open(os.path.join(self.image_directory, image_path))\n",
    "        label = self.age_class_to_label[age_class]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_CHECKPOINTS = Path('./checkpoints')\n",
    "PATH_TO_CHECKPOINTS.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7giqCBhGsnJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_to_age = {\n",
    "    0: \"0-6 years old\",\n",
    "    1: \"7-12 years old\",\n",
    "    2: \"13-19 years old\",\n",
    "    3: \"20-30 years old\",\n",
    "    4: \"31-45 years old\",\n",
    "    5: \"46-55 years old\",\n",
    "    6: \"56-66 years old\",\n",
    "    7: \"67-80 years old\"\n",
    "}\n",
    "\n",
    "train_meta_data_path = \"./custom_korean_family_dataset_resolution_128/new_custom_train_dataset.csv\"\n",
    "train_meta_data = pd.read_csv(train_meta_data_path)\n",
    "train_image_directory = \"./custom_korean_family_dataset_resolution_128/train_images\"\n",
    "\n",
    "shadow_meta_data_dir = Path(\"./custom_korean_family_dataset_resolution_128/custom_shadow_dataset\")\n",
    "\n",
    "test_meta_data_path = \"./custom_korean_family_dataset_resolution_128/custom_val_dataset.csv\"\n",
    "test_meta_data = pd.read_csv(test_meta_data_path)\n",
    "test_image_directory = \"./custom_korean_family_dataset_resolution_128/val_images\"\n",
    "\n",
    "unseen_meta_data_path = \"./custom_korean_family_dataset_resolution_128/custom_test_dataset.csv\"\n",
    "unseen_meta_data = pd.read_csv(unseen_meta_data_path)\n",
    "unseen_image_directory = \"./custom_korean_family_dataset_resolution_128/test_images\"\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "unseen_transform = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = Dataset(train_meta_data, train_image_directory, train_transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = Dataset(test_meta_data, test_image_directory, test_transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "unseen_dataset = Dataset(unseen_meta_data, unseen_image_directory, unseen_transform)\n",
    "unseen_dataloader = DataLoader(unseen_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "1XRp73a8HJgr",
    "outputId": "2cef9b1e-dc3f-4257-dcc2-492c9955f64e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_images(images, labels, nrow=6, save_path=None):\n",
    "    n_images = len(images)\n",
    "    nrows = n_images // nrow + (n_images % nrow > 0)\n",
    "\n",
    "    fig, axs = plt.subplots(nrows, nrow, figsize=(14.5, 2.3 * nrows), frameon=False)\n",
    "    axs = axs.flatten() if n_images > 1 else [axs]\n",
    "\n",
    "    for idx, (img, label) in enumerate(zip(images, labels)):\n",
    "        ax = axs[idx]\n",
    "        img_np = img.numpy().transpose((1, 2, 0))\n",
    "        ax.imshow(img_np)\n",
    "        ax.axis('off')\n",
    "\n",
    "        ax.text(5, 5, label, color='white', fontsize=13,  ha='left', va='top',\n",
    "                bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.1'))\n",
    "\n",
    "    plt.tight_layout(pad=0)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "iterator = iter(test_dataloader)\n",
    "imgs, labels = next(iterator)\n",
    "\n",
    "label_strs = [label_to_age[label.item()] for label in labels[7:19]]\n",
    "\n",
    "show_images(imgs[7:19], label_strs, nrow=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nck1gGsvITcS"
   },
   "source": [
    "#### <b>Train the \"Original Model\"</b>\n",
    "\n",
    "* Train the Original model to serve as the base model for performing Machine Unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-qsC5RlYIJwh",
    "outputId": "afe18052-c62e-4932-c345-f0cc47705ffd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "log_step = 30\n",
    "\n",
    "model = models.resnet18(pretrained=False)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 8)\n",
    "model = model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sf_vtq7uIjWH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    start_time = time.time()\n",
    "    print(f'[Epoch: {epoch + 1} - Training]')\n",
    "    model.train()\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        imgs, labels = batch\n",
    "        imgs, labels = imgs.cuda(), labels.cuda()\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        optimizer.zero_grad()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += labels.shape[0]\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        if i % log_step == log_step - 1:\n",
    "            print(f'[Batch: {i + 1}] running train loss: {running_loss / total}, running train accuracy: {running_corrects / total}')\n",
    "\n",
    "    print(f'train loss: {running_loss / total}, accuracy: {running_corrects / total}')\n",
    "    print(\"elapsed time:\", time.time() - start_time)\n",
    "    return running_loss / total, (running_corrects / total).item()\n",
    "\n",
    "\n",
    "def test():\n",
    "    start_time = time.time()\n",
    "    print(f'[Test]')\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        imgs, labels = batch\n",
    "        imgs, labels = imgs.cuda(), labels.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(imgs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        total += labels.shape[0]\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        if (i == 0) or (i % log_step == log_step - 1):\n",
    "            print(f'[Batch: {i + 1}] running test loss: {running_loss / total}, running test accuracy: {running_corrects / total}')\n",
    "\n",
    "    print(f'test loss: {running_loss / total}, accuracy: {running_corrects / total}')\n",
    "    print(\"elapsed time:\", time.time() - start_time)\n",
    "    return running_loss / total, (running_corrects / total).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzstS9NMKeGA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate\n",
    "    if epoch >= 10:\n",
    "        lr /= 10\n",
    "    if epoch >= 20:\n",
    "        lr /= 10\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "num_original_epochs = 30\n",
    "best_test_acc = 0\n",
    "best_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "iXKsCFIXlY3m",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_random_seed(42)\n",
    "\n",
    "path_to_target_checkpoint = PATH_TO_CHECKPOINTS/'target'\n",
    "path_to_target_checkpoint.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "history = []\n",
    "accuracy = []\n",
    "for epoch in range(num_original_epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train_loss, train_acc = train()\n",
    "    test_loss, test_acc = test()\n",
    "    history.append((train_loss, test_loss))\n",
    "    accuracy.append((train_acc, test_acc))\n",
    "\n",
    "torch.save(model.state_dict(), path_to_target_checkpoint/'resnet18.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bfJ_FFUm2wd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot([x[0] for x in history], 'b', label='train')\n",
    "plt.plot([x[1] for x in history], 'r--',label='test')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7o06syjmN32u",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot([x[0] for x in accuracy], 'b', label='train')\n",
    "plt.plot([x[1] for x in accuracy], 'r--',label='test')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b> Train the \"Shadow Models\"</b>\n",
    "* The models which will be used for MIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_shadow(model):\n",
    "    start_time = time.time()\n",
    "    print(f'[Epoch: {epoch + 1} - Training]')\n",
    "    model.train()\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for i, batch in enumerate(shadow_dataloader):\n",
    "        imgs, labels = batch\n",
    "        imgs, labels = imgs.cuda(), labels.cuda()\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        optimizer.zero_grad()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += labels.shape[0]\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        if i % log_step == log_step - 1:\n",
    "            print(f'[Batch: {i + 1}] running train loss: {running_loss / total}, running train accuracy: {running_corrects / total}')\n",
    "\n",
    "    print(f'train loss: {running_loss / total}, accuracy: {running_corrects / total}')\n",
    "    print(\"elapsed time:\", time.time() - start_time)\n",
    "    return running_loss / total, (running_corrects / total).item()\n",
    "\n",
    "\n",
    "def test():\n",
    "    start_time = time.time()\n",
    "    print(f'[Test]')\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        imgs, labels = batch\n",
    "        imgs, labels = imgs.cuda(), labels.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(imgs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        total += labels.shape[0]\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        if (i == 0) or (i % log_step == log_step - 1):\n",
    "            print(f'[Batch: {i + 1}] running test loss: {running_loss / total}, running test accuracy: {running_corrects / total}')\n",
    "\n",
    "    print(f'test loss: {running_loss / total}, accuracy: {running_corrects / total}')\n",
    "    print(\"elapsed time:\", time.time() - start_time)\n",
    "    return running_loss / total, (running_corrects / total).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_shadow_checkpoint = PATH_TO_CHECKPOINTS/'shadow'\n",
    "path_to_shadow_checkpoint.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "shadow_inds = sorted([int(x.split('.')[0]) for x in os.listdir(shadow_meta_data_dir)])\n",
    "\n",
    "for shadow_idx in shadow_inds:\n",
    "\n",
    "    set_random_seed(shadow_idx)\n",
    "\n",
    "    shadow_meta_data = pd.read_csv(shadow_meta_data_dir/f'{shadow_idx:04}.csv')\n",
    "    shadow_dataset = Dataset(shadow_meta_data, train_image_directory, train_transform)\n",
    "    shadow_dataloader = DataLoader(shadow_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    for epoch in range(num_original_epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        train_loss, train_acc = train()\n",
    "        test_loss, test_acc = test()\n",
    "        history.append((train_loss, test_loss))\n",
    "        accuracy.append((train_acc, test_acc))\n",
    "\n",
    "    torch.save(model.state_dict(), path_to_shadow_checkpoint/f'resnet18_{shadow_idx:04}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from MIA.utils import load_model, load_shadow_models\n",
    "from MIA.score import get_logits, lira_offline\n",
    "\n",
    "\n",
    "def lira(model):\n",
    "    num_aug = 1\n",
    "    fix_variance = False\n",
    "\n",
    "    transform = train_transform if num_aug > 1 else test_transform\n",
    "    dataset = Dataset(train_meta_data, train_image_directory, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    shadow_nets = load_shadow_models()\n",
    "    target_net = load_model('./checkpoints/target/resnet18.pth')\n",
    "\n",
    "    shadow_logits = []\n",
    "    target_logits = []\n",
    "    dataset_labels = []\n",
    "\n",
    "    for _ in range(num_aug):\n",
    "        shadow_aug_logits = [[] for _ in range(len(shadow_nets))]\n",
    "        target_aug_logits = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                # 1. Store shadow logits\n",
    "                for i, net in enumerate(shadow_nets):\n",
    "                    logits = net(inputs).cpu().tolist()\n",
    "                    shadow_aug_logits[i].append(logits)\n",
    "                # 2. Store target logits\n",
    "                logits = target_net(inputs).cpu().tolist()\n",
    "                target_aug_logits.append(logits)\n",
    "            \n",
    "                dataset_labels.append(targets.cpu().tolist())\n",
    "\n",
    "        shadow_aug_logits = np.stack(\n",
    "            [np.concatenate(logits) for logits in shadow_aug_logits])  # [n_shadow, n_examples, n_classes]\n",
    "        target_aug_logits = np.concatenate(target_aug_logits)  # [n_examples, n_classes]\n",
    "        dataset_labels = np.concatenate(dataset_labels)[:, None]  # [n_examples, 1]\n",
    "\n",
    "        # Extract numerically stable logits\n",
    "        shadow_aug_logits = get_logits(shadow_aug_logits, np.repeat(dataset_labels[None], len(shadow_nets), 0))\n",
    "        target_aug_logits = get_logits(target_aug_logits, dataset_labels)\n",
    "\n",
    "        shadow_logits.append(shadow_aug_logits)\n",
    "        target_logits.append(target_aug_logits)\n",
    "\n",
    "        shadow_logits = np.stack(shadow_logits, axis=-1)\n",
    "        shadow_logits = np.swapaxes(shadow_logits, 0, 1)  # [n_examples, n_shadow, n_aug]\n",
    "        target_logits = np.stack(target_logits, axis=-1)  # [n_examples, n_aug]\n",
    "        labels = np.ones(target_logits.shape[0]) # [n_examples]\n",
    "\n",
    "        fpr, tpr, auc, acc, low = lira_offline(target_logits, shadow_logits, labels, fix_variance=fix_variance)\n",
    "\n",
    "    return fpr, tpr, auc, acc, low "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8urhpGvSkjL"
   },
   "source": [
    "#### <b> Train the \"Retrained Model\"</b>\n",
    "* A model trained on only the retain dataset from scratch, excluding the forget dataset.\n",
    "* This model can serve as a proxy ground-truth for calculating the potential ground-truth (1) model utility and (2) forgetting score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### <b>Load Datasets 2</b>\n",
    "\n",
    "* In the NeurIPS Competition setting, when performing machine unlearning, training is done with 'retain' and 'forget', so augmentation can be applied. However, it should never be applied during evaluation.\n",
    "* We divide training dataset $\\mathcal{D}_{train}$ into a retain dataset $\\mathcal{D}_{retain}$ and a forget dataset $\\mathcal{D}_{forget}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forget_dataset_train = Dataset(train_meta_data, train_image_directory, train_transform, forget=True)\n",
    "forget_dataloader_train = DataLoader(forget_dataset_train, batch_size=64, shuffle=True)\n",
    "\n",
    "retain_dataset_train = Dataset(train_meta_data, train_image_directory, train_transform, retain=True)\n",
    "retain_dataloader_train = DataLoader(retain_dataset_train, batch_size=64, shuffle=True)\n",
    "\n",
    "forget_dataset_test = Dataset(train_meta_data, train_image_directory, test_transform, forget=True)\n",
    "forget_dataloader_test = DataLoader(forget_dataset_test, batch_size=64, shuffle=False)\n",
    "\n",
    "retain_dataset_test = Dataset(train_meta_data, train_image_directory, test_transform, retain=True)\n",
    "retain_dataloader_test = DataLoader(retain_dataset_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Train dataset size:', len(train_dataset))\n",
    "print('Test dataset size:', len(test_dataset))\n",
    "print('Forget dataset size:', len(forget_dataset_train))\n",
    "print('Retain dataset size:', len(retain_dataset_train))\n",
    "print('Unseen dataset size:', len(unseen_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking for duplicates across datasets\n",
    "train_image_paths = [x[0] for x in train_dataset.image_age_list]\n",
    "test_image_paths = [x[0] for x in test_dataset.image_age_list]\n",
    "forget_image_paths = [x[0] for x in forget_dataset_train.image_age_list]\n",
    "retain_image_paths = [x[0] for x in retain_dataset_train.image_age_list]\n",
    "unseen_image_paths = [x[0] for x in unseen_dataset.image_age_list]\n",
    "\n",
    "train_test_overlap = len(set(train_image_paths) & set(test_image_paths))\n",
    "train_forget_overlap = len(set(train_image_paths) & set(forget_image_paths))\n",
    "train_retain_overlap = len(set(train_image_paths) & set(retain_image_paths))\n",
    "train_unseen_overlap = len(set(train_image_paths) & set(unseen_image_paths))\n",
    "test_forget_overlap = len(set(test_image_paths) & set(forget_image_paths))\n",
    "test_retain_overlap = len(set(test_image_paths) & set(retain_image_paths))\n",
    "test_unseen_overlap = len(set(test_image_paths) & set(unseen_image_paths))\n",
    "forget_retain_overlap = len(set(forget_image_paths) & set(retain_image_paths))\n",
    "forget_unseen_overlap = len(set(forget_image_paths) & set(unseen_image_paths))\n",
    "retain_unseen_overlap = len(set(retain_image_paths) & set(unseen_image_paths))\n",
    "\n",
    "overlap_dict = {\n",
    "    'train_test': train_test_overlap,\n",
    "    'train_forget': train_forget_overlap,\n",
    "    'train_retain': train_retain_overlap,\n",
    "    'train_unseen': train_unseen_overlap,\n",
    "    'test_forget': test_forget_overlap,\n",
    "    'test_retain': test_retain_overlap,\n",
    "    'test_unseen': test_unseen_overlap,\n",
    "    'forget_retain': forget_retain_overlap,\n",
    "    'forget_unseen': forget_unseen_overlap,\n",
    "    'retain_unseen': retain_unseen_overlap\n",
    "}\n",
    "overlap_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGI6rxao3RKQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluation(model, data_loader):\n",
    "    start_time = time.time()\n",
    "    print(f'[Test]')\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    running_top2_corrects = 0\n",
    "\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        imgs, labels = batch\n",
    "        imgs, labels = imgs.cuda(), labels.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(imgs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Top-2 accuracy.\n",
    "            _, top2_preds = outputs.topk(2, dim=1)  # Get the top 2 class indices.\n",
    "            top2_correct = top2_preds.eq(labels.view(-1, 1).expand_as(top2_preds))\n",
    "            running_top2_corrects += top2_correct.any(dim=1).sum().item()\n",
    "\n",
    "        total += labels.shape[0]\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data).item()\n",
    "\n",
    "        if (i == 0) or (i % log_step == log_step - 1):\n",
    "            print(f'[Batch: {i + 1}] running test loss: {running_loss / total}, running test accuracy: {running_corrects / total}, running top-2 accuracy: {running_top2_corrects / total}')\n",
    "\n",
    "    print(f'test loss: {running_loss / total}, accuracy: {running_corrects / total}, top-2 accuracy: {running_top2_corrects / total}')\n",
    "    print(\"elapsed time:\", time.time() - start_time)\n",
    "    return {'Loss': running_loss / total, 'Acc': running_corrects / total, 'Top-2 Acc': running_top2_corrects / total}\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PX5rU0Tb2gxE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_losses(net, loader):\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    all_losses = []\n",
    "\n",
    "    for inputs, y in loader:\n",
    "        targets = y\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        logits = net(inputs)\n",
    "\n",
    "        losses = criterion(logits, targets).cpu().detach().numpy()\n",
    "        for l in losses:\n",
    "            all_losses.append(l)\n",
    "\n",
    "    return np.array(all_losses)\n",
    "\n",
    "def simple_mia(sample_loss, members, n_splits=10, random_state=0):\n",
    "    unique_members = np.unique(members)\n",
    "    if not np.all(unique_members == np.array([0, 1])):\n",
    "        raise ValueError(\"members should only have 0 and 1s\")\n",
    "\n",
    "    attack_model = linear_model.LogisticRegression()\n",
    "    cv = model_selection.StratifiedShuffleSplit(\n",
    "        n_splits=n_splits, random_state=random_state\n",
    "    )\n",
    "    return model_selection.cross_val_score(\n",
    "        attack_model, sample_loss, members, cv=cv, scoring=\"accuracy\"\n",
    "    )\n",
    "\n",
    "def cal_mia(model):\n",
    "    set_seed(42)\n",
    "\n",
    "    forget_losses = compute_losses(model, forget_dataloader_test)\n",
    "    unseen_losses = compute_losses(model, unseen_dataloader)\n",
    "\n",
    "    np.random.shuffle(forget_losses)\n",
    "    forget_losses = forget_losses[: len(unseen_losses)]\n",
    "\n",
    "    samples_mia = np.concatenate((unseen_losses, forget_losses)).reshape((-1, 1))\n",
    "    labels_mia = [0] * len(unseen_losses) + [1] * len(forget_losses)\n",
    "\n",
    "    mia_scores = simple_mia(samples_mia, labels_mia)\n",
    "    forgetting_score = abs(0.5 - mia_scores.mean())\n",
    "\n",
    "    return {'MIA': mia_scores.mean(), 'Forgeting Score': forgetting_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AKOp9kZ8Sjdo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model_using_only_retrain(model):\n",
    "    start_time = time.time()\n",
    "    print(f'[Epoch: {epoch + 1} - Training]')\n",
    "    model.train()\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for i, batch in enumerate(retain_dataloader_train):\n",
    "        imgs, labels = batch\n",
    "        imgs, labels = imgs.cuda(), labels.cuda()\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        optimizer.zero_grad()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += labels.shape[0]\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        if i % log_step == log_step - 1:\n",
    "            print(f'[Batch: {i + 1}] running train loss: {running_loss / total}, running train accuracy: {running_corrects / total}')\n",
    "\n",
    "    print(f'train loss: {running_loss / total}, accuracy: {running_corrects / total}')\n",
    "    print(\"elapsed time:\", time.time() - start_time)\n",
    "    return running_loss / total, (running_corrects / total).item()\n",
    "\n",
    "def test_model(model):\n",
    "    start_time = time.time()\n",
    "    print(f'[Test]')\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        imgs, labels = batch\n",
    "        imgs, labels = imgs.cuda(), labels.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(imgs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        total += labels.shape[0]\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        if (i == 0) or (i % log_step == log_step - 1):\n",
    "            print(f'[Batch: {i + 1}] running test loss: {running_loss / total}, running test accuracy: {running_corrects / total}')\n",
    "\n",
    "    print(f'test loss: {running_loss / total}, accuracy: {running_corrects / total}')\n",
    "    print(\"elapsed time:\", time.time() - start_time)\n",
    "    return running_loss / total, (running_corrects / total).item()\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate\n",
    "    if epoch >= 10:\n",
    "        lr /= 10\n",
    "    if epoch >= 20:\n",
    "        lr /= 10\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "WbI3p0jy2obD",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3bc7421c-eea5-432d-af77-2afe8f00c366",
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "num_epochs = 30\n",
    "best_test_acc = 0\n",
    "best_epoch = 0\n",
    "learning_rate = 0.01\n",
    "\n",
    "retrained_model = models.resnet18(pretrained=False)\n",
    "num_features = retrained_model.fc.in_features\n",
    "retrained_model.fc = nn.Linear(num_features, 8)\n",
    "retrained_model = retrained_model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(retrained_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "history = []\n",
    "accuracy = []\n",
    "for epoch in range(num_epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train_loss, train_acc = train_model_using_only_retrain(retrained_model)\n",
    "    test_loss, test_acc = test_model(retrained_model)\n",
    "    history.append((train_loss, test_loss))\n",
    "    accuracy.append((train_acc, test_acc))\n",
    "\n",
    "    if test_acc > best_test_acc:\n",
    "        print(\"[Info] best test accuracy!\")\n",
    "        best_test_acc = test_acc\n",
    "        best_epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCp5iMhrEO4D",
    "outputId": "a8f99b9d-b4be-44b1-e4ca-c9863c398169",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Performance\n",
    "test_acc = evaluation(retrained_model, test_dataloader)\n",
    "unseen_acc = evaluation(retrained_model, unseen_dataloader)\n",
    "mia = cal_mia(retrained_model.cuda())\n",
    "print(f'Test Acc: {test_acc}')\n",
    "print(f'Unseen Acc: {unseen_acc}')\n",
    "print(f'MIA: {mia}')\n",
    "print()\n",
    "print(f'Final Score: {(test_acc[\"Acc\"] + (1 - abs(mia[\"MIA\"] - 0.5) * 2)) / 2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrE8vlhLb8ex"
   },
   "source": [
    "#### <b>Machine Unlearning Experiments</b>\n",
    "\n",
    "* In the NeurIPS Competition setting, when performing machine unlearning, training is done with 'retain' and 'forget', so augmentation can be applied. However, it should never be applied during evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JOesO7Q15lU"
   },
   "source": [
    "<b>Original Model Preformance</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k5Cx69Nh18Nk",
    "outputId": "4bca7d46-4e16-444a-b2c3-49e7c66e4638",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f'last_checkpoint_epoch_{num_original_epochs}.pth')\n",
    "# original_save_path = f'last_checkpoint_epoch_{num_original_epochs}.pth' # If you trian the original model from scratch.\n",
    "original_save_path = f'pre_trained_last_checkpoint_epoch_30.pth'\n",
    "original_model = models.resnet18(pretrained=False)\n",
    "num_features = original_model.fc.in_features\n",
    "original_model.fc = nn.Linear(num_features, 8)\n",
    "original_model.load_state_dict(torch.load(original_save_path))\n",
    "original_model = original_model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "test_acc = evaluation(original_model, test_dataloader)\n",
    "test_acc\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Performance\n",
    "test_acc = evaluation(original_model, test_dataloader)\n",
    "unseen_acc = evaluation(original_model, unseen_dataloader)\n",
    "mia = cal_mia(original_model.cuda())\n",
    "print()\n",
    "print(f'Test Acc: {test_acc}')\n",
    "print(f'Unseen Acc: {unseen_acc}')\n",
    "print(f'MIA: {mia}')\n",
    "print(f'Final Score: {(test_acc[\"Acc\"] + (1 - abs(mia[\"MIA\"] - 0.5) * 2)) / 2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MR1GH-Q2Uuj"
   },
   "source": [
    "#### <b>Fine-Tuning</b>\n",
    "* A method where the original model undergoes further training solely on the retain dataset.\n",
    "* This approach aims to reinforce the model's knowledge of the retain dataset, potentially leading to the forgetting of the previously learned forget dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dCN-IdL120-L",
    "outputId": "f9b80e6f-caad-41f9-ab2b-e8f742f24a88",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f'last_checkpoint_epoch_{num_original_epochs}.pth')\n",
    "# original_save_path = f'last_checkpoint_epoch_{num_original_epochs}.pth' # If you trian the original model from scratch.\n",
    "original_save_path = f'pre_trained_last_checkpoint_epoch_30.pth'\n",
    "unlearned_model = models.resnet18(pretrained=False)\n",
    "num_features = unlearned_model.fc.in_features\n",
    "unlearned_model.fc = nn.Linear(num_features, 8)\n",
    "unlearned_model.load_state_dict(torch.load(original_save_path))\n",
    "unlearned_model = unlearned_model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "test_acc = evaluation(unlearned_model, test_dataloader)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J7RHWNPo2WKb",
    "outputId": "911609a7-4d9e-4fa1-c5f5-0924d4c979c3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(unlearned_model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "\n",
    "    for batch_idx, (x_retain, y_retain) in enumerate(retain_dataloader_train):\n",
    "        y_retain = y_retain.cuda()\n",
    "\n",
    "        # Classification Loss\n",
    "        outputs_retain = unlearned_model(x_retain.cuda())\n",
    "        classification_loss = criterion(outputs_retain, y_retain)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        classification_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += classification_loss.item() * x_retain.size(0)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(retain_dataloader_train)}] - Batch Loss: {classification_loss.item():.4f}\")\n",
    "\n",
    "    average_epoch_loss = running_loss / (len(retain_dataloader_train) * x_retain.size(0))\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Total Loss: {running_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v_2ynMWE2nh0",
    "outputId": "b58bc8f5-a9f0-45ba-fe3e-ad4a2f2355f5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Performance\n",
    "test_acc = evaluation(unlearned_model, test_dataloader)\n",
    "unseen_acc = evaluation(unlearned_model, unseen_dataloader)\n",
    "mia = cal_mia(unlearned_model.cuda())\n",
    "print()\n",
    "print(f'Test Acc: {test_acc}')\n",
    "print(f'Unseen Acc: {unseen_acc}')\n",
    "print(f'MIA: {mia}')\n",
    "print(f'Final Score: {(test_acc[\"Acc\"] + (1 - abs(mia[\"MIA\"] - 0.5) * 2)) / 2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcoOFrkpz-7Q"
   },
   "source": [
    "#### <b>CF-k (Class-wise Forgetting)</b>\n",
    "* A strategy that fine-tunes specific layers to make the model forget target data while retaining class structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5nN7uYsx0bp_",
    "outputId": "f29ff72f-9efd-4e05-c07a-6a7990d50964",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f'last_checkpoint_epoch_{num_original_epochs}.pth')\n",
    "# original_save_path = f'last_checkpoint_epoch_{num_original_epochs}.pth' # If you trian the original model from scratch.\n",
    "original_save_path = f'pre_trained_last_checkpoint_epoch_30.pth'\n",
    "unlearned_model = models.resnet18(pretrained=False)\n",
    "num_features = unlearned_model.fc.in_features\n",
    "unlearned_model.fc = nn.Linear(num_features, 8)\n",
    "unlearned_model.load_state_dict(torch.load(original_save_path))\n",
    "cf_model = unlearned_model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6HZNTXYHodf6",
    "outputId": "7d018ab7-9ad0-4a02-92d6-3813eab5b027",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print a weight set of a front layer in the network to check the freezing.\n",
    "print(unlearned_model.layer1[0].conv1.weight[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DJcE3M71dj3O",
    "outputId": "4127ec04-962b-4a3c-bc73-945c5358d83b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, unlearned_model.parameters()), lr=0.001)\n",
    "\n",
    "# Freeze all the parameters.\n",
    "for param in unlearned_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Only unfreeze the last three layers for the fine-tuning.\n",
    "for param in unlearned_model.layer3.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in unlearned_model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in unlearned_model.avgpool.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in unlearned_model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "\n",
    "    for batch_idx, (x_retain, y_retain) in enumerate(retain_dataloader_train):\n",
    "        y_retain = y_retain.cuda()\n",
    "\n",
    "        # Classification Loss\n",
    "        outputs_retain = unlearned_model(x_retain.cuda())\n",
    "        classification_loss = criterion(outputs_retain, y_retain)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        classification_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += classification_loss.item() * x_retain.size(0)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(retain_dataloader_train)}] - Batch Loss: {classification_loss.item():.4f}\")\n",
    "\n",
    "    average_epoch_loss = running_loss / (len(retain_dataloader_train) * x_retain.size(0))\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Total Loss: {running_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K4cmQ9IC1-eS",
    "outputId": "9836a48a-4346-4e52-cf87-f96686405ee7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Performance\n",
    "test_acc = evaluation(unlearned_model, test_dataloader)\n",
    "unseen_acc = evaluation(unlearned_model, unseen_dataloader)\n",
    "mia = cal_mia(unlearned_model.cuda())\n",
    "print(f'Test Acc: {test_acc}')\n",
    "print(f'Unseen Acc: {unseen_acc}')\n",
    "print(f'MIA: {mia}')\n",
    "print(f'Final Score: {(test_acc[\"Acc\"] + (1 - abs(mia[\"MIA\"] - 0.5) * 2)) / 2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HdS9DY2zpO9i",
    "outputId": "dca2baf9-7f51-4ec7-89b3-8af96bde4aac",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print a weight set of a front layer in the network to check the freezing.\n",
    "print(unlearned_model.layer1[0].conv1.weight[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIPumLGo2xBU"
   },
   "source": [
    "#### <b>Negative Gradient Ascent</b>\n",
    "* A strategy that induces the model to 'forget' particular data by deliberately maximizing the error on the forget dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "112NBbuDqpuh"
   },
   "source": [
    "<b>Normal NegGrad</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B_0E6EZJ2XNC",
    "outputId": "dc1f2be5-62ab-41c3-e02c-eb9e1b8d309d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f'last_checkpoint_epoch_{num_original_epochs}.pth')\n",
    "# original_save_path = f'last_checkpoint_epoch_{num_original_epochs}.pth' # If you trian the original model from scratch.\n",
    "original_save_path = f'pre_trained_last_checkpoint_epoch_30.pth'\n",
    "unlearned_model = models.resnet18(pretrained=False)\n",
    "num_features = unlearned_model.fc.in_features\n",
    "unlearned_model.fc = nn.Linear(num_features, 8)\n",
    "unlearned_model.load_state_dict(torch.load(original_save_path))\n",
    "unlearned_model = unlearned_model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B29QV5f3qpDR",
    "outputId": "184ff7ac-1aad-4389-d293-8fe28a917ded",
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(unlearned_model.parameters(), lr=0.001)\n",
    "\n",
    "dataloader_iterator = iter(forget_dataloader_train)\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "\n",
    "    for batch_idx, (x_retain, y_retain) in enumerate(retain_dataloader_train):\n",
    "        y_retain = y_retain.cuda()\n",
    "\n",
    "        try:\n",
    "            (x_forget, y_forget) = next(dataloader_iterator)\n",
    "        except StopIteration:\n",
    "            dataloader_iterator = iter(forget_dataloader_train)\n",
    "            (x_forget, y_forget) = next(dataloader_iterator)\n",
    "\n",
    "        if x_forget.size(0) != x_retain.size(0):\n",
    "            continue\n",
    "\n",
    "        outputs_forget = unlearned_model(x_forget.cuda())\n",
    "        loss_ascent_forget = -criterion(outputs_forget, y_forget.cuda())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_ascent_forget.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss_ascent_forget.item() * x_retain.size(0)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(retain_dataloader_train)}] - Batch Loss: {loss_ascent_forget.item():.4f}\")\n",
    "\n",
    "    average_epoch_loss = running_loss / (len(retain_dataloader_train) * x_retain.size(0))\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Total Loss: {running_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u1gtVguFrQR1",
    "outputId": "dc0e3ff1-63d7-4e97-b549-805b212b6d32",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Performance\n",
    "test_acc = evaluation(unlearned_model, test_dataloader)\n",
    "unseen_acc = evaluation(unlearned_model, unseen_dataloader)\n",
    "mia = cal_mia(unlearned_model.cuda())\n",
    "print(f'Test Acc: {test_acc}')\n",
    "print(f'Unseen Acc: {unseen_acc}')\n",
    "print(f'MIA: {mia}')\n",
    "print(f'Final Score: {(test_acc[\"Acc\"] + (1 - abs(mia[\"MIA\"] - 0.5) * 2)) / 2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZaJVVRerB-Z"
   },
   "source": [
    "<b>Advanced NegGrad with Classification Loss</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AKanMIqdrK1V",
    "outputId": "5755a57f-ed61-418e-f59b-58a14a8f8f88",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f'last_checkpoint_epoch_{num_original_epochs}.pth')\n",
    "# original_save_path = f'last_checkpoint_epoch_{num_original_epochs}.pth' # If you trian the original model from scratch.\n",
    "original_save_path = f'pre_trained_last_checkpoint_epoch_30.pth'\n",
    "unlearned_model = models.resnet18(pretrained=False)\n",
    "num_features = unlearned_model.fc.in_features\n",
    "unlearned_model.fc = nn.Linear(num_features, 8)\n",
    "unlearned_model.load_state_dict(torch.load(original_save_path))\n",
    "unlearned_model = unlearned_model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "kfeuPLoN22pV",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "bc0a8ce6-5409-46a0-e4cf-66657a3f4ba1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(unlearned_model.parameters(), lr=0.001)\n",
    "\n",
    "dataloader_iterator = iter(forget_dataloader_train)\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "\n",
    "    for batch_idx, (x_retain, y_retain) in enumerate(retain_dataloader_train):\n",
    "        y_retain = y_retain.cuda()\n",
    "\n",
    "        try:\n",
    "            (x_forget, y_forget) = next(dataloader_iterator)\n",
    "        except StopIteration:\n",
    "            dataloader_iterator = iter(forget_dataloader_train)\n",
    "            (x_forget, y_forget) = next(dataloader_iterator)\n",
    "\n",
    "        if x_forget.size(0) != x_retain.size(0):\n",
    "            continue\n",
    "\n",
    "        outputs_retain = unlearned_model(x_retain.cuda())\n",
    "        outputs_forget = unlearned_model(x_forget.cuda())\n",
    "\n",
    "        loss_ascent_forget = -criterion(outputs_forget, y_forget.cuda())\n",
    "        loss_retain = criterion(outputs_retain, y_retain.cuda())\n",
    "\n",
    "        # Overall loss\n",
    "        joint_loss = loss_ascent_forget + loss_retain\n",
    "\n",
    "        print(\"joint loss :\", joint_loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        joint_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += joint_loss.item() * x_retain.size(0)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(retain_dataloader_train)}] - Batch Loss: {joint_loss.item():.4f}\")\n",
    "\n",
    "    average_epoch_loss = running_loss / (len(retain_dataloader_train) * x_retain.size(0))\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Total Loss: {running_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5wunEIMA-Sxf",
    "outputId": "98ff6f66-c8cf-460a-f4f1-c5f8145b5411",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Performance\n",
    "test_acc = evaluation(unlearned_model, test_dataloader)\n",
    "unseen_acc = evaluation(unlearned_model, unseen_dataloader)\n",
    "mia = cal_mia(unlearned_model.cuda())\n",
    "print(f'Test Acc: {test_acc}')\n",
    "print(f'Unseen Acc: {unseen_acc}')\n",
    "print(f'MIA: {mia}')\n",
    "print(f'Final Score: {(test_acc[\"Acc\"] + (1 - abs(mia[\"MIA\"] - 0.5) * 2)) / 2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8CW5VIwclHe"
   },
   "source": [
    "#### <b>UNSIR</b>\n",
    "* <b>Impair phase (Stage 1)</b>: Update noise to increase the distance between the model and the forget dataset. Then, the updated noise is then integrated into the training dataset to enhance the model's ability to forget the specific dataset.\n",
    "* <b>Repair phase (Stage 2)</b>: Repair the impaired model using the retain dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wpaf9nUt-nPM"
   },
   "source": [
    "<b> Stage1: Impair</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Z8EsL1CcKhY",
    "outputId": "e32911de-de6d-4149-f93e-24fb7fe14f8e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f'last_checkpoint_epoch_{num_original_epochs}.pth')\n",
    "# original_save_path = f'last_checkpoint_epoch_{num_original_epochs}.pth' # If you trian the original model from scratch.\n",
    "original_save_path = f'pre_trained_last_checkpoint_epoch_30.pth'\n",
    "unlearned_model = models.resnet18(pretrained=False)\n",
    "num_features = unlearned_model.fc.in_features\n",
    "unlearned_model.fc = nn.Linear(num_features, 8)\n",
    "unlearned_model.load_state_dict(torch.load(original_save_path))\n",
    "unlearned_model = unlearned_model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "test_acc = evaluation(unlearned_model, test_dataloader)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1npMwrjb_t3Z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Noise(nn.Module):\n",
    "    def __init__(self, batch_size, *dim):\n",
    "        super().__init__()\n",
    "        self.noise = nn.Parameter(torch.randn(batch_size, *dim), requires_grad=True)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.noise\n",
    "\n",
    "def float_to_uint8(img_float):\n",
    "    \"\"\"Convert a floating point image in the range [0,1] to uint8 image in the range [0,255].\"\"\"\n",
    "    img_uint8 = (img_float * 255).astype(np.uint8)\n",
    "    return img_uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ooPshXSoST3B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(unlearned_model.parameters(), lr=0.001)\n",
    "\n",
    "print_interval = 1\n",
    "train_epoch_losses = []\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "\n",
    "    for batch_idx, ((x_retain, y_retain), (x_forget, y_forget)) in enumerate(zip(retain_dataloader_train, forget_dataloader_train)):\n",
    "        y_retain = y_retain.cuda()\n",
    "        batch_size_forget = y_forget.size(0)\n",
    "\n",
    "        if x_retain.size(0) != 64 or x_forget.size(0) != 64:\n",
    "            continue\n",
    "\n",
    "        # Initialize the noise.\n",
    "        noise_dim = x_retain.size(1), x_retain.size(2), x_retain.size(3)\n",
    "        noise = Noise(batch_size_forget, *noise_dim).cuda()\n",
    "        noise_optimizer = torch.optim.Adam(noise.parameters(), lr=0.01)\n",
    "        noise_tensor = noise()[:batch_size_forget]\n",
    "\n",
    "        # Update the noise for increasing the loss value.\n",
    "        for _ in range(5):\n",
    "            outputs = unlearned_model(noise_tensor)\n",
    "            with torch.no_grad():\n",
    "                target_logits = unlearned_model(x_forget.cuda())\n",
    "            # Maximize the similarity between noise data and forget features.\n",
    "            loss_noise = -F.mse_loss(outputs, target_logits)\n",
    "\n",
    "            # Backpropagate to update the noise.\n",
    "            noise_optimizer.zero_grad()\n",
    "            loss_noise.backward(retain_graph=True)\n",
    "            noise_optimizer.step()\n",
    "\n",
    "        # Train the model with noise and retain image\n",
    "        noise_tensor = torch.clamp(noise_tensor, 0, 1).detach().cuda()\n",
    "        outputs = unlearned_model(noise_tensor.cuda())\n",
    "        loss_1 = criterion(outputs, y_retain)\n",
    "\n",
    "        outputs = unlearned_model(x_retain.cuda())\n",
    "        loss_2 = criterion(outputs, y_retain)\n",
    "\n",
    "        joint_loss = loss_1 + loss_2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        joint_loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += joint_loss.item() * x_retain.size(0)\n",
    "\n",
    "        original_image = x_retain[0].cpu().numpy().transpose(1, 2, 0)\n",
    "        image1 = TF.to_pil_image(float_to_uint8(original_image))\n",
    "        image2 = TF.to_pil_image(noise.noise[0].cpu())\n",
    "\n",
    "        # Display original image.\n",
    "        original_image = x_retain[0].cpu().numpy().transpose(1, 2, 0)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.subplot(1, 2, 1)  # 2 rows, 2 columns, position 1\n",
    "        plt.imshow(image1)\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Display first noise image.\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(image2)\n",
    "        plt.title(\"Noise Image\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Show all the subplots.\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        if batch_idx % print_interval == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(retain_dataloader_train)}] - Batch Loss: {joint_loss.item():.4f}\")\n",
    "\n",
    "    average_train_loss = running_loss / (len(retain_dataloader_train) * x_retain.size(0))\n",
    "    train_epoch_losses.append(average_train_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {average_train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vUZFqwJgdA94",
    "outputId": "d0fdc7b7-abee-46c5-88a1-44ea395d6dc0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Performance\n",
    "test_acc = evaluation(unlearned_model, test_dataloader)\n",
    "unseen_acc = evaluation(unlearned_model, unseen_dataloader)\n",
    "mia = cal_mia(unlearned_model.cuda())\n",
    "print()\n",
    "print(f'Test Acc: {test_acc}')\n",
    "print(f'Unseen Acc: {unseen_acc}')\n",
    "print(f'MIA: {mia}')\n",
    "print(f'Final Score: {(test_acc[\"Acc\"] + (1 - abs(mia[\"MIA\"] - 0.5) * 2)) / 2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYFdj391LL-k"
   },
   "source": [
    "<b>Stage 2: Repair</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bW6cCwXHLC8j",
    "outputId": "5e1681c3-e7c9-4cba-95b8-d6e938d49433",
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(unlearned_model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "\n",
    "    for batch_idx, (x_retain, y_retain) in enumerate(retain_dataloader_train):\n",
    "        y_retain = y_retain.cuda()\n",
    "\n",
    "        # Classification Loss\n",
    "        outputs_retain = unlearned_model(x_retain.cuda())\n",
    "        classification_loss = criterion(outputs_retain, y_retain)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        classification_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += classification_loss.item() * x_retain.size(0)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(retain_dataloader_train)}] - Batch Loss: {classification_loss.item():.4f}\")\n",
    "\n",
    "    average_epoch_loss = running_loss / (len(retain_dataloader_train) * x_retain.size(0))\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Total Loss: {running_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4k0Q5hcQLDBA",
    "outputId": "81608598-dc7f-4a1c-f1a7-4c9fa98b8027",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Performance\n",
    "test_acc = evaluation(unlearned_model, test_dataloader)\n",
    "unseen_acc = evaluation(unlearned_model, unseen_dataloader)\n",
    "mia = cal_mia(unlearned_model.cuda())\n",
    "print()\n",
    "print(f'Test Acc: {test_acc}')\n",
    "print(f'Unseen Acc: {unseen_acc}')\n",
    "print(f'MIA: {mia}')\n",
    "print(f'Final Score: {(test_acc[\"Acc\"] + (1 - abs(mia[\"MIA\"] - 0.5) * 2)) / 2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR2u3wGk5cOe"
   },
   "source": [
    "#### <b>Scrub</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4YqUI6k6E3M",
    "outputId": "2926ca6e-3368-4cf8-99fe-70fda57a101c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f'last_checkpoint_epoch_{num_original_epochs}.pth')\n",
    "# original_save_path = f'last_checkpoint_epoch_{num_original_epochs}.pth' # If you trian the original model from scratch.\n",
    "original_save_path = f'pre_trained_last_checkpoint_epoch_30.pth'\n",
    "original_model = models.resnet18(pretrained=False)\n",
    "num_features = original_model.fc.in_features\n",
    "original_model.fc = nn.Linear(num_features, 8)\n",
    "original_model.load_state_dict(torch.load(original_save_path))\n",
    "original_model = original_model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tffmxVAt5bDH",
    "outputId": "c23bfc1a-a129-47bd-cb15-ac0d2a401822",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f'last_checkpoint_epoch_{num_original_epochs}.pth')\n",
    "# original_save_path = f'last_checkpoint_epoch_{num_original_epochs}.pth' # If you trian the original model from scratch.\n",
    "original_save_path = f'pre_trained_last_checkpoint_epoch_30.pth'\n",
    "scrub_model = models.resnet18(pretrained=False)\n",
    "num_features = scrub_model.fc.in_features\n",
    "scrub_model.fc = nn.Linear(num_features, 8)\n",
    "scrub_model.load_state_dict(torch.load(original_save_path))\n",
    "scrub_model = scrub_model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATgoLi1x5m7W",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DistillKL(nn.Module):\n",
    "    def __init__(self, T):\n",
    "        super(DistillKL, self).__init__()\n",
    "        self.T = T\n",
    "\n",
    "    def forward(self, y_s, y_t):\n",
    "        p_s = F.log_softmax(y_s/self.T, dim=1)\n",
    "        p_t = F.softmax(y_t/self.T, dim=1)\n",
    "        loss = F.kl_div(p_s, p_t, size_average=False) * (self.T**2) / y_s.shape[0]\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMGQ42t35qVl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SCRUBTraining:\n",
    "    def __init__(self, teacher, student, retain_dataloader, forget_dataloader):\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "        self.retain_dataloader = retain_dataloader\n",
    "        self.forget_dataloader = forget_dataloader\n",
    "\n",
    "        self.criterion_cls = nn.CrossEntropyLoss()\n",
    "        self.criterion_div = DistillKL(4.0)\n",
    "        self.criterion_kd = DistillKL(4.0)\n",
    "\n",
    "        self.optimizer = optim.SGD(student.parameters(), lr=0.001)\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.student.train()\n",
    "        self.teacher.eval()\n",
    "\n",
    "        # Function to compute accuracy.\n",
    "        def compute_accuracy(outputs, labels):\n",
    "            _, predicted = outputs.max(1)\n",
    "            total = labels.size(0)\n",
    "            correct = predicted.eq(labels).sum().item()\n",
    "            return 100 * correct / total\n",
    "\n",
    "        total_loss_retain, total_accuracy_retain = 0, 0\n",
    "        total_loss_forget, total_accuracy_forget = 0, 0\n",
    "\n",
    "        # Training with retain data.\n",
    "        for inputs_retain, labels_retain in self.retain_dataloader:\n",
    "            inputs_retain, labels_retain = inputs_retain.cuda(), labels_retain.cuda()\n",
    "\n",
    "            # Forward pass: Student\n",
    "            outputs_retain_student = self.student(inputs_retain)\n",
    "\n",
    "            # Forward pass: Teacher\n",
    "            with torch.no_grad():\n",
    "                outputs_retain_teacher = self.teacher(inputs_retain)\n",
    "\n",
    "            # Loss computation\n",
    "            loss_cls = self.criterion_cls(outputs_retain_student, labels_retain)\n",
    "            loss_div_retain = self.criterion_div(outputs_retain_student, outputs_retain_teacher)\n",
    "\n",
    "            loss = loss_cls + loss_div_retain\n",
    "\n",
    "            # Update total loss and accuracy for retain data.\n",
    "            total_loss_retain += loss.item()\n",
    "            total_accuracy_retain += compute_accuracy(outputs_retain_student, labels_retain)\n",
    "\n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Training with forget data.\n",
    "        for inputs_forget, labels_forget in self.forget_dataloader:\n",
    "            inputs_forget, labels_forget = inputs_forget.cuda(), labels_forget.cuda()\n",
    "\n",
    "            # Forward pass: Student\n",
    "            outputs_forget_student = self.student(inputs_forget)\n",
    "\n",
    "            # Forward pass: Teacher\n",
    "            with torch.no_grad():\n",
    "                outputs_forget_teacher = self.teacher(inputs_forget)\n",
    "\n",
    "            # We want to maximize the divergence for the forget data.\n",
    "            loss_div_forget = -self.criterion_div(outputs_forget_student, outputs_forget_teacher)\n",
    "\n",
    "            # Update total loss and accuracy for forget data.\n",
    "            total_loss_forget += loss_div_forget.item()\n",
    "            total_accuracy_forget += compute_accuracy(outputs_forget_student, labels_forget)\n",
    "\n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss_div_forget.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Print average loss and accuracy for the entire epoch\n",
    "        avg_loss_retain = total_loss_retain / len(self.retain_dataloader)\n",
    "        avg_accuracy_retain = total_accuracy_retain / len(self.retain_dataloader)\n",
    "\n",
    "        avg_loss_forget = total_loss_forget / len(self.forget_dataloader)\n",
    "        avg_accuracy_forget = total_accuracy_forget / len(self.forget_dataloader)\n",
    "\n",
    "        print(f'Epoch Retain: Avg Loss: {avg_loss_retain:.4f}, Avg Accuracy: {avg_accuracy_retain:.2f}%')\n",
    "        print(f'Epoch Forget: Avg Loss: {avg_loss_forget:.4f}, Avg Accuracy: {avg_accuracy_forget:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6mILWEn6AeG",
    "outputId": "5a655406-5459-4bb5-d824-b1a3f975ef85",
    "tags": []
   },
   "outputs": [],
   "source": [
    "teacher = original_model\n",
    "student = scrub_model\n",
    "\n",
    "# Initialize and train\n",
    "scrub_trainer = SCRUBTraining(teacher, student, retain_dataloader_train, forget_dataloader_train)\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    scrub_trainer.train_epoch()\n",
    "    print(f\"Epoch {epoch+1} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qYZIlHN_6ifE",
    "outputId": "aef21e08-b4e5-44dc-b289-7378215727ca",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Performance\n",
    "test_acc = evaluation(scrub_model, test_dataloader)\n",
    "unseen_acc = evaluation(scrub_model, unseen_dataloader)\n",
    "mia = cal_mia(scrub_model.cuda())\n",
    "print(f'Test Acc: {test_acc}')\n",
    "print(f'Unseen Acc: {unseen_acc}')\n",
    "print(f'MIA: {mia}')\n",
    "print(f'Final Score: {(test_acc[\"Acc\"] + (1 - abs(mia[\"MIA\"] - 0.5) * 2)) / 2}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "zhavoronkin_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
